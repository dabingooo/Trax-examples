{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trax-transformer-将各种日期格式转为标准格式",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dabingooo/Trax-examples/blob/master/trax_transformer_%E5%B0%86%E5%90%84%E7%A7%8D%E6%97%A5%E6%9C%9F%E6%A0%BC%E5%BC%8F%E8%BD%AC%E4%B8%BA%E6%A0%87%E5%87%86%E6%A0%BC%E5%BC%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vrpm1As8YV0"
      },
      "source": [
        "# 将各种日期格式转为标准格式 by Trax Transformer模型\n",
        "本代码是作为使用google的trax库的例子. 本来是打算做机器翻译(中文->英文), 但因为资源不足,训练时间长,且很可能没有好的结果.所以将问题简化为将各种日期格式->标准日期的训练.\n",
        "\n",
        "**转化示例**:\n",
        "> 各种日期格式&nbsp;&nbsp;&nbsp;->&nbsp;&nbsp;标准日期格式<br/>\n",
        "'10.11.19'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;->&nbsp;&nbsp;'2019-11-10'<br/>\n",
        "'1970/9/10'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;->&nbsp;&nbsp;'1970-09-10'<br/>\n",
        "'1990年4月28日星期六'&nbsp;&nbsp;->&nbsp;&nbsp;'1990-04-28'<br/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "vlGjGoGMTt-D",
        "outputId": "2585c6c9-3d20-4363-f2d3-db30ae3608a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "!pip install -q -U trax\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "!pip install -q faker\n",
        "!pip install -q tqdm\n",
        "!pip install -q babel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 419kB 5.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 12.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 45.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 53.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 62.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 12.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 39.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 63.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 63.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 63.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 655kB 63.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 307kB 55.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 55.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5MB 59.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 52.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 64.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 56.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 46.3MB/s \n",
            "\u001b[?25h  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: kfac 0.2.2 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n",
            "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 9.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8caYzOV27tiW"
      },
      "source": [
        "下面函数用于生成数据.此代码源自吴恩达Deep-Learning-Specialization-Coursera课程,此处代码拷贝自[Here](https://github.com/AdalbertoCq/Deep-Learning-Specialization-Coursera/blob/master/Sequence%20Models/week3/Neural%20machine%20translation%20with%20attention/nmt_utils.py)\n",
        ",有一些小改动.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBnRYAP9xtmY"
      },
      "source": [
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "\n",
        "fake = Faker()\n",
        "Faker.seed(12345)\n",
        "random.seed(12345)\n",
        "\n",
        "# Define format of the data we would like to generate\n",
        "FORMATS = ['short',\n",
        "           'medium',\n",
        "           'long',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'd MMM YYY', \n",
        "           'd MMMM YYY',\n",
        "           'dd MMM YYY',\n",
        "           'd MMM, YYY',\n",
        "           'd MMMM, YYY',\n",
        "           'dd, MMM YYY',\n",
        "           'd MM YY',\n",
        "           'd MMMM YYY',\n",
        "           'MMMM d YYY',\n",
        "           'MMMM d, YYY',\n",
        "           'dd.MM.YY']\n",
        "\n",
        "# change this if you want it to work with another language\n",
        "LOCALES = ['en_US']\n",
        "\n",
        "def load_date():\n",
        "    \"\"\"\n",
        "        Loads some fake dates \n",
        "        :returns: tuple containing human readable string, machine readable string, and date object\n",
        "    \"\"\"\n",
        "    dt = fake.date_object()\n",
        "\n",
        "    try:\n",
        "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='zh_CN') # locale=random.choice(LOCALES))\n",
        "        human_readable = human_readable.lower()\n",
        "        human_readable = human_readable.replace(',','')\n",
        "        machine_readable = dt.isoformat()\n",
        "        \n",
        "    except AttributeError as e:\n",
        "        return None, None, None\n",
        "\n",
        "    return human_readable, machine_readable, dt\n",
        "\n",
        "def load_dataset(m):\n",
        "    \"\"\"\n",
        "        Loads a dataset with m examples and vocabularies\n",
        "        :m: the number of examples to generate\n",
        "    \"\"\"\n",
        "    \n",
        "    human_vocab = set()\n",
        "    machine_vocab = set()\n",
        "    dataset = []\n",
        "    Tx = 30\n",
        "    \n",
        "    for i in tqdm(range(m)):\n",
        "        h, m, _ = load_date()\n",
        "        if h is not None:\n",
        "            dataset.append((h, m))\n",
        "            human_vocab.update(tuple(h))\n",
        "            machine_vocab.update(tuple(m))\n",
        "    \n",
        "    human = dict(zip(['@', '#'] + sorted(human_vocab), \n",
        "                     list(range(len(human_vocab) + 2))))\n",
        "    inv_machine = dict(zip(['@', '#'] + sorted(machine_vocab), \n",
        "                     list(range(len(machine_vocab) + 2))))\n",
        "    machine = {v:k for k,v in inv_machine.items()}\n",
        " \n",
        "    return dataset, human, machine, inv_machine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoPn7hNaBwl-"
      },
      "source": [
        "生成训练数据集:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIlev3b61BHc",
        "outputId": "d5afc235-4248-4853-80ca-81be2b23c96e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "m = 120000    # 数据总数量\n",
        "m_train = 100000 # 用于训练的数量,其余的用于测试时的验证\n",
        "max_len = 32\n",
        "batch_size = 32\n",
        "dataset, human_vocab, inv_machine_vocab, machine_vocab = load_dataset(m)\n",
        "\n",
        "print('\\n========数据集前10个=========')\n",
        "print(dataset[:10])\n",
        "print('========可读日期vocab=========')\n",
        "print(human_vocab)\n",
        "print('========机器日期vocab=========')\n",
        "print(machine_vocab)\n",
        "print('========机器日期 反vocab=========')\n",
        "print(inv_machine_vocab)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120000/120000 [00:19<00:00, 6139.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "========数据集前10个=========\n",
            "[('9 5月 1998', '1998-05-09'), ('10.11.19', '2019-11-10'), ('1970/9/10', '1970-09-10'), ('1990年4月28日星期六', '1990-04-28'), ('1995年1月26日星期四', '1995-01-26'), ('1983年3月7日星期一', '1983-03-07'), ('1988年5月22日星期日', '1988-05-22'), ('08 7月 2008', '2008-07-08'), ('8 9月 1999', '1999-09-08'), ('1981年1月1日星期四', '1981-01-01')]\n",
            "========可读日期vocab=========\n",
            "{'@': 0, '#': 1, ' ': 2, '.': 3, '/': 4, '0': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '8': 13, '9': 14, '一': 15, '七': 16, '三': 17, '九': 18, '二': 19, '五': 20, '八': 21, '六': 22, '十': 23, '四': 24, '年': 25, '日': 26, '星': 27, '月': 28, '期': 29}\n",
            "========机器日期vocab=========\n",
            "{'@': 0, '#': 1, '-': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12}\n",
            "========机器日期 反vocab=========\n",
            "{0: '@', 1: '#', 2: '-', 3: '0', 4: '1', 5: '2', 6: '3', 7: '4', 8: '5', 9: '6', 10: '7', 11: '8', 12: '9'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ill1rn-Cpvb"
      },
      "source": [
        "因为trax中的tokenize()和detokenize()函数,如果词汇表类型(vocab_type)使用'char'方式是不能指定自己词汇表(指定了内部也不用).为了使用自己的词汇表所以自己简单实现tokenize()和detokenize()功能:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEQjrLH3cP5s",
        "outputId": "0b8a9163-f4a0-4216-8127-b9b0a055d52d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "def _tok(data, dic):\n",
        "  '''\n",
        "  将字符串转为整数token\n",
        "  data: String类型\n",
        "  dic: 词汇表字典\n",
        "  return: np.array类型\n",
        "  '''\n",
        "  s = []\n",
        "  for c in data:\n",
        "    s.append(dic[c])\n",
        "  return np.array(s)\n",
        "\n",
        "def tok_tuple_list(data, dic, axis=0):\n",
        "  '''\n",
        "  将 列表中的每个tuplet中的字符串 tokenize\n",
        "  data: tuple列表, 如[('abc', 'ABC'), ('def', 'fukk')]\n",
        "  dic: 词汇表字典\n",
        "  axis: 指定tuple中第几个元素执行tokenize\n",
        "  return: tokenize后的tuple列表, 如[(np.array([1, 3]), 'ABC'), (np.array([1, 3]), 'fukk')]\n",
        "  ''' \n",
        "  l = []\n",
        "  for da in data:\n",
        "    l.append(_tok(da[axis], dic))\n",
        "  one, two  = zip(*data)\n",
        "  if axis==0:\n",
        "    return list(zip(l, two))   \n",
        "  return list(zip(one, l))\n",
        "\n",
        "def tok_string_list(data, dic):\n",
        "  '''\n",
        "  将 字符串列表 转为 token列表\n",
        "  data: 字符串列表, 如['abc', 'def']\n",
        "  dic: 词汇表字典\n",
        "  return: token列表, 如[np.array([1, 3]), np.array([1, 3])]\n",
        "  ''' \n",
        "  l = []\n",
        "  for da in data:\n",
        "    l.append(_tok(da, dic))  \n",
        "  return l\n",
        "\n",
        "def detok_string_list(data, dic):\n",
        "  '''\n",
        "  将 token列表 转为 字符串列表\n",
        "  data: token列表, 如[np.array([1, 3]), np.array([1, 3])]\n",
        "  dic: 词汇表字典\n",
        "  return: 字符串列表, 如['abc', 'def']\n",
        "  ''' \n",
        "  l = ['' for i in range(data.shape[0])]\n",
        "  for i in range(data.shape[0]):\n",
        "    for j in range(data.shape[1]):\n",
        "      l[i] += dic[data[i][j]]\n",
        "  return l\n",
        "\n",
        "print(f'数据集前2个数据: {dataset[:2]}')\n",
        "input = tok_tuple_list(dataset, human_vocab, 0)\n",
        "input = tok_tuple_list(input, machine_vocab, 1)\n",
        "print(f'数据集前2个数据token: {input[:2]}')\n",
        "\n",
        "#unit test\n",
        "print('===============unit test: detok_string_list() and detok_string_list()===========================')\n",
        "tmp_d = ['1998-05-09', '2019-11-10']\n",
        "print(f'source: {tmp_d}')\n",
        "tmp_tok = tok_string_list(tmp_d, machine_vocab)\n",
        "print(f'source_tok: {tmp_tok}')\n",
        "tmp_tok = np.concatenate((tmp_tok[0], tmp_tok[1])).reshape(2, len(tmp_tok[0]))\n",
        "source_new = detok_string_list(tmp_tok, inv_machine_vocab)\n",
        "print(f'source_new: {source_new}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "数据集前2个数据: [('9 5月 1998', '1998-05-09'), ('10.11.19', '2019-11-10')]\n",
            "数据集前2个数据token: [(array([14,  2, 10, 28,  2,  6, 14, 14, 13]), array([ 4, 12, 12, 11,  2,  3,  8,  2,  3, 12])), (array([ 6,  5,  3,  6,  6,  3,  6, 14]), array([ 5,  3,  4, 12,  2,  4,  4,  2,  4,  3]))]\n",
            "===============unit test: detok_string_list() and detok_string_list()===========================\n",
            "source: ['1998-05-09', '2019-11-10']\n",
            "source_tok: [array([ 4, 12, 12, 11,  2,  3,  8,  2,  3, 12]), array([ 5,  3,  4, 12,  2,  4,  4,  2,  4,  3])]\n",
            "source_new: ['1998-05-09', '2019-11-10']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NC2DOn5Ekis"
      },
      "source": [
        "数据预处理:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqi66QVzqwhV",
        "outputId": "737faf3d-e325-4c1b-badb-103b00d88ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "input_pip = trax.data.Serial(\n",
        "  trax.data.FilterByLength(max_length=max_len, length_keys=[0, 1]),\n",
        "  trax.data.BucketByLength(boundaries=[max_len],\n",
        "              batch_sizes=[batch_size, 1],\n",
        "              length_keys=[0, 1],\n",
        "              strict_pad_on_len=True),  \n",
        "  trax.data.AddLossWeights(id_to_mask=0),\n",
        ")\n",
        "\n",
        "train_batches_stream = input_pip(input[:m_train])\n",
        "eval_batches_stream = input_pip(input[m_train:])\n",
        "\n",
        "o = next(train_batches_stream)\n",
        "print(f'第一批数据:{o}')\n",
        "print(f'第一批数据tuple中每个数据shape: {[x.shape for x in o]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "第一批数据:(array([[14,  2, 10, ...,  0,  0,  0],\n",
            "       [ 6,  5,  3, ...,  0,  0,  0],\n",
            "       [ 6, 14, 12, ...,  0,  0,  0],\n",
            "       ...,\n",
            "       [ 6, 14, 13, ...,  0,  0,  0],\n",
            "       [ 6, 14, 13, ...,  0,  0,  0],\n",
            "       [ 6, 14, 13, ...,  0,  0,  0]]), array([[ 4, 12, 12, ...,  0,  0,  0],\n",
            "       [ 5,  3,  4, ...,  0,  0,  0],\n",
            "       [ 4, 12, 10, ...,  0,  0,  0],\n",
            "       ...,\n",
            "       [ 4, 12, 11, ...,  0,  0,  0],\n",
            "       [ 4, 12, 11, ...,  0,  0,  0],\n",
            "       [ 4, 12, 11, ...,  0,  0,  0]]), array([[1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.]], dtype=float32))\n",
            "第一批数据tuple中每个数据shape: [(32, 32), (32, 32), (32, 32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUyINmPXE-EF"
      },
      "source": [
        "创建Transformer训练模型:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoSz5plIyXOU"
      },
      "source": [
        "# MODEL\n",
        "def create_model(mode = 'train'):\n",
        "  return trax.models.Transformer(\n",
        "      input_vocab_size=len(human_vocab),\n",
        "      output_vocab_size=len(machine_vocab),\n",
        "      d_model=32, d_ff=128,\n",
        "      n_heads=8, n_encoder_layers=2, n_decoder_layers=2,\n",
        "      max_len=max_len, mode=mode)\n",
        "\n",
        "# UNUSED\n",
        "def create_model_reformer(mode = 'train'):\n",
        "  return trax.models.Reformer(input_vocab_size=8269,\n",
        "              output_vocab_size=8185,\n",
        "              d_model=256,\n",
        "              d_ff=1024,\n",
        "              n_encoder_layers=2,\n",
        "              n_decoder_layers=2,\n",
        "              n_heads=8,\n",
        "              dropout=0.1,\n",
        "              max_len=258,\n",
        "              ff_activation=tl.Relu,\n",
        "              ff_dropout=None,\n",
        "              mode=mode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TVx_BccFA6c"
      },
      "source": [
        "训练(使用GPU可以加快训练速度):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6bIKUO-3Cw8",
        "outputId": "c5dbc300-df9b-4aab-cc69-e1fc3fc01140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        }
      },
      "source": [
        "# TRAIN\n",
        "from trax.supervised import training\n",
        "\n",
        "# Training task.\n",
        "train_task = training.TrainTask(\n",
        "    labeled_data=train_batches_stream,\n",
        "    loss_layer=tl.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adafactor(),\n",
        "    #optimizer=trax.optimizers.Adam(learning_rate=0.1, weight_decay_rate=1e-05, b1=0.9, b2=0.98, eps=1e-06, clip_grad_norm=None),\n",
        "    n_steps_per_checkpoint=300,\n",
        ")\n",
        "\n",
        "# Evaluaton task.\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data=eval_batches_stream,\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        "    n_eval_batches=20  # For less variance in eval numbers.\n",
        ")\n",
        "\n",
        "# Training loop saves checkpoints to output_dir.\n",
        "output_dir = os.path.expanduser('~/output_dir/')\n",
        "print(output_dir)\n",
        "!rm -rf {output_dir}\n",
        "training_loop = training.Loop(create_model(),\n",
        "                train_task,\n",
        "                eval_tasks=[eval_task],\n",
        "                output_dir=output_dir)\n",
        "\n",
        "\n",
        "# Run\n",
        "training_loop.run(1800)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/output_dir/\n",
            "\n",
            "Step      1: Ran 1 train steps in 16.44 secs\n",
            "Step      1: train CrossEntropyLoss |  3.02210116\n",
            "Step      1: eval  CrossEntropyLoss |  2.46161259\n",
            "Step      1: eval          Accuracy |  0.20125000\n",
            "\n",
            "Step    300: Ran 299 train steps in 18.27 secs\n",
            "Step    300: train CrossEntropyLoss |  0.90951586\n",
            "Step    300: eval  CrossEntropyLoss |  0.68087703\n",
            "Step    300: eval          Accuracy |  0.74750000\n",
            "\n",
            "Step    600: Ran 300 train steps in 4.32 secs\n",
            "Step    600: train CrossEntropyLoss |  0.37855753\n",
            "Step    600: eval  CrossEntropyLoss |  0.12818675\n",
            "Step    600: eval          Accuracy |  0.95859376\n",
            "\n",
            "Step    900: Ran 300 train steps in 4.37 secs\n",
            "Step    900: train CrossEntropyLoss |  0.08506602\n",
            "Step    900: eval  CrossEntropyLoss |  0.06627027\n",
            "Step    900: eval          Accuracy |  0.97968751\n",
            "\n",
            "Step   1200: Ran 300 train steps in 4.35 secs\n",
            "Step   1200: train CrossEntropyLoss |  0.04304006\n",
            "Step   1200: eval  CrossEntropyLoss |  0.08948441\n",
            "Step   1200: eval          Accuracy |  0.97468751\n",
            "\n",
            "Step   1500: Ran 300 train steps in 4.33 secs\n",
            "Step   1500: train CrossEntropyLoss |  0.03000528\n",
            "Step   1500: eval  CrossEntropyLoss |  0.04981535\n",
            "Step   1500: eval          Accuracy |  0.98781251\n",
            "\n",
            "Step   1800: Ran 300 train steps in 4.37 secs\n",
            "Step   1800: train CrossEntropyLoss |  0.02536340\n",
            "Step   1800: eval  CrossEntropyLoss |  0.05158633\n",
            "Step   1800: eval          Accuracy |  0.98921876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtrqgebEM60O"
      },
      "source": [
        "测试模型效果:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B98rTQXeb2UJ",
        "outputId": "46916175-5895-4017-ebff-394fd622e0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 生成测试数据\n",
        "test_dataset, _, _, _ = load_dataset(10)\n",
        "\n",
        "for dat in test_dataset:\n",
        "  # 模型必须每次重新加载, 因为调用autoregressive_sample()时,会改变model的状态值\n",
        "  model = create_model('predict') \n",
        "  model.init_from_file(output_dir+ '/model.pkl.gz', weights_only=True, \n",
        "            input_signature=[trax.shapes.ShapeDtype((1, 1), np.int32), \n",
        "                      trax.shapes.ShapeDtype((1, 1), np.int32),\n",
        "                      trax.shapes.ShapeDtype((1, 1), np.float32)])\n",
        "  # Tokenize a sentence.\n",
        "  test_source = [dat[0]]\n",
        "  test_target = [dat[1]]\n",
        "  test_tok = tok_string_list(test_source, human_vocab)\n",
        "  test_tok = test_tok[0].reshape(1, len(test_tok[0]))\n",
        "  #test_tok = trax.data.inputs.pad_to_max_dims(test_tok, boundary=32, strict_pad_on_len=True)\n",
        "\n",
        "  res_tok = trax.supervised.decoding.autoregressive_sample(\n",
        "      model, inputs=test_tok, batch_size=1, temperature=0.5, \n",
        "      start_id=human_vocab['@'], eos_id=machine_vocab['#'], max_length=10, accelerate=False)\n",
        "\n",
        "  res = detok_string_list(res_tok, inv_machine_vocab)\n",
        "  print('======================================================')\n",
        "  print(f'输入: {test_source}')\n",
        "  print(f'token(输入): {test_tok}')\n",
        "  print(f'token(输出): {res_tok}')\n",
        "  print(f'输出: {res}')\n",
        "  print(f'真值: {test_target}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 2755.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "======================================================\n",
            "输入: ['2002年12月3日星期二']\n",
            "token(输入): [[ 7  5  5  7 25  6  7 28  8 26 27 29 19]]\n",
            "token(输出): [[5 3 3 5 2 4 5 2 3 6]]\n",
            "输出: ['2002-12-03']\n",
            "真值: ['2002-12-03']\n",
            "======================================================\n",
            "输入: ['1990/3/30']\n",
            "token(输入): [[ 6 14 14  5  4  8  4  8  5]]\n",
            "token(输出): [[ 4 12 12  3  2  3  6  2  6  3]]\n",
            "输出: ['1990-03-30']\n",
            "真值: ['1990-03-30']\n",
            "======================================================\n",
            "输入: ['3 十一月 1991']\n",
            "token(输入): [[ 8  2 23 15 28  2  6 14 14  6]]\n",
            "token(输出): [[ 4 12 12  4  2  4  4  2  3 12]]\n",
            "输出: ['1991-11-09']\n",
            "真值: ['1991-11-03']\n",
            "======================================================\n",
            "输入: ['1989年12月21日星期四']\n",
            "token(输入): [[ 6 14 13 14 25  6  7 28  7  6 26 27 29 24]]\n",
            "token(输出): [[ 4 12 11 12  2  4  5  2  5  4]]\n",
            "输出: ['1989-12-21']\n",
            "真值: ['1989-12-21']\n",
            "======================================================\n",
            "输入: ['5 十一月 2011']\n",
            "token(输入): [[10  2 23 15 28  2  7  5  6  6]]\n",
            "token(输出): [[5 3 4 4 2 4 4 2 3 8]]\n",
            "输出: ['2011-11-05']\n",
            "真值: ['2011-11-05']\n",
            "======================================================\n",
            "输入: ['2000年7月30日星期日']\n",
            "token(输入): [[ 7  5  5  5 25 12 28  8  5 26 27 29 26]]\n",
            "token(输出): [[ 5  3  3  3  2  3 10  2  6  3]]\n",
            "输出: ['2000-07-30']\n",
            "真值: ['2000-07-30']\n",
            "======================================================\n",
            "输入: ['八月 5 2016']\n",
            "token(输入): [[21 28  2 10  2  7  5  6 11]]\n",
            "token(输出): [[ 5  3  4  9  2  3 11  2  3  8]]\n",
            "输出: ['2016-08-05']\n",
            "真值: ['2016-08-05']\n",
            "======================================================\n",
            "输入: ['2010年2月2日星期二']\n",
            "token(输入): [[ 7  5  6  5 25  7 28  7 26 27 29 19]]\n",
            "token(输出): [[5 3 4 3 2 3 5 2 3 5]]\n",
            "输出: ['2010-02-02']\n",
            "真值: ['2010-02-02']\n",
            "======================================================\n",
            "输入: ['25 2月 2004']\n",
            "token(输入): [[ 7 10  2  7 28  2  7  5  5  9]]\n",
            "token(输出): [[5 3 3 7 2 3 5 2 5 8]]\n",
            "输出: ['2004-02-25']\n",
            "真值: ['2004-02-25']\n",
            "======================================================\n",
            "输入: ['1997年9月17日星期三']\n",
            "token(输入): [[ 6 14 14 12 25 14 28  6 12 26 27 29 17]]\n",
            "token(输出): [[ 4 12 12 10  2  3 12  2  4 10]]\n",
            "输出: ['1997-09-17']\n",
            "真值: ['1997-09-17']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}