{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trax-transformer-将各种日期格式转为标准格式",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dabingooo/Trax-examples/blob/master/trax_transformer_%E5%B0%86%E5%90%84%E7%A7%8D%E6%97%A5%E6%9C%9F%E6%A0%BC%E5%BC%8F%E8%BD%AC%E4%B8%BA%E6%A0%87%E5%87%86%E6%A0%BC%E5%BC%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IgvbESdY_U_"
      },
      "source": [
        "# 将各种日期格式转为标准格式 by Trax Transformer模型\n",
        "本代码是作为使用google的trax库的例子. 本来是打算做机器翻译(中文->英文), 但因为资源不足,训练时间长,且很可能没有好的结果.所以将问题简化为将各种日期格式->标准日期的训练.\n",
        "\n",
        "**转化示例**:\n",
        "> 各种日期格式&nbsp;&nbsp;&nbsp;->&nbsp;&nbsp;标准日期格式<br/>\n",
        "'10.11.19'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;->&nbsp;&nbsp;'2019-11-10'<br/>\n",
        "'1970/9/10'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;->&nbsp;&nbsp;'1970-09-10'<br/>\n",
        "'1990年4月28日星期六'&nbsp;&nbsp;->&nbsp;&nbsp;'1990-04-28'<br/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "vlGjGoGMTt-D"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "!pip install -q -U trax\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "!pip install -q faker\n",
        "!pip install -q tqdm\n",
        "!pip install -q babel"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8caYzOV27tiW"
      },
      "source": [
        "下面函数用于生成数据.此代码源自吴恩达Deep-Learning-Specialization-Coursera课程,此处代码拷贝自[Here](https://github.com/AdalbertoCq/Deep-Learning-Specialization-Coursera/blob/master/Sequence%20Models/week3/Neural%20machine%20translation%20with%20attention/nmt_utils.py)\n",
        ",有较大改动.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBnRYAP9xtmY",
        "outputId": "8183fbf9-a8cb-4b01-ae53-03ffb8abbbe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "\n",
        "fake = Faker()\n",
        "Faker.seed(12345)\n",
        "random.seed(12345)\n",
        "\n",
        "# Define format of the data we would like to generate\n",
        "FORMATS = ['short',\n",
        "           'medium',\n",
        "           'long',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'd MMM YYY', \n",
        "           'd MMMM YYY',\n",
        "           'dd MMM YYY',\n",
        "           'd MMM, YYY',\n",
        "           'd MMMM, YYY',\n",
        "           'dd, MMM YYY',\n",
        "           'd MM YY',\n",
        "           'd MMMM YYY',\n",
        "           'MMMM d YYY',\n",
        "           'MMMM d, YYY',\n",
        "           'dd.MM.YY']\n",
        "\n",
        "# change this if you want it to work with another language\n",
        "LOCALES = ['en_US']\n",
        "\n",
        "def load_date():\n",
        "    \"\"\"\n",
        "        Loads some fake dates \n",
        "        :returns: tuple containing human readable string, machine readable string, and date object\n",
        "    \"\"\"\n",
        "    dt = fake.date_object()\n",
        "\n",
        "    try:\n",
        "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='zh_CN') # locale=random.choice(LOCALES))\n",
        "        human_readable = human_readable.lower()\n",
        "        human_readable = human_readable.replace(',','')\n",
        "        machine_readable = dt.isoformat()\n",
        "        \n",
        "    except AttributeError as e:\n",
        "        return None, None, None\n",
        "\n",
        "    return human_readable, machine_readable, dt\n",
        "\n",
        "def load_dataset_yield():\n",
        "  \"\"\"\n",
        "    Loads a dataset with m examples and vocabularies\n",
        "    :m: the number of examples to generate\n",
        "  \"\"\"       \n",
        "  while True:\n",
        "    h, m, _ = load_date()\n",
        "    if h is not None:\n",
        "      h += '#'\n",
        "      m += '#'\n",
        "      yield (h, m)\n",
        "\n",
        "print('示例数据:')\n",
        "print(next(load_dataset_yield()))\n",
        "print(next(load_dataset_yield()))\n",
        "print(next(load_dataset_yield()))\n",
        "\n",
        "#========可读日期vocab=========    \n",
        "human_vocab = {'@': 0, '#': 1, ' ': 2, '.': 3, '/': 4, '0': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '8': 13, '9': 14, '一': 15, '七': 16, '三': 17, '九': 18, '二': 19, '五': 20, '八': 21, '六': 22, '十': 23, '四': 24, '年': 25, '日': 26, '星': 27, '月': 28, '期': 29}\n",
        "#========机器日期vocab=========\n",
        "machine_vocab = {'@': 0, '#': 1, '-': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12}\n",
        "#========机器日期 反vocab=========\n",
        "inv_machine_vocab = {0: '@', 1: '#', 2: '-', 3: '0', 4: '1', 5: '2', 6: '3', 7: '4', 8: '5', 9: '6', 10: '7', 11: '8', 12: '9'}    \n",
        "\n",
        "max_len = 32\n",
        "batch_size = 64"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "示例数据:\n",
            "('9 5月 1998#', '1998-05-09#')\n",
            "('10.11.19#', '2019-11-10#')\n",
            "('1970/9/10#', '1970-09-10#')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ill1rn-Cpvb"
      },
      "source": [
        "因为trax中的tokenize()和detokenize()函数,如果词汇表类型(vocab_type)使用'char'方式是不能指定自己词汇表(指定了内部也不用).为了使用自己的词汇表所以自己简单实现tokenize()和detokenize()功能:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEQjrLH3cP5s"
      },
      "source": [
        "def tok(data, dic):\n",
        "  '''\n",
        "  将字符串转为整数token\n",
        "  data: String类型\n",
        "  dic: 词汇表字典\n",
        "  return: np.array类型\n",
        "  '''\n",
        "  s = []\n",
        "  for c in data:\n",
        "    s.append(dic[c])\n",
        "  return np.array(s) \n",
        "\n",
        "def detok(data, dic):\n",
        "  '''\n",
        "  将 token(np.array类型) 转为 字符串/列表\n",
        "  data: token, 如 np.array.shape = (batch_size, seq_length)\n",
        "  dic: 词汇表字典\n",
        "  return: 字符串/列表, 如'abc' 或 ['abc', 'def']\n",
        "  ''' \n",
        "  if len(data.shape)>2:\n",
        "    raise ValueError(f'The dim of input can NOT > 2. the dim of input is {len(data.shape)}')\n",
        "  data = data if len(data.shape) > 1 else data[None, :]\n",
        "  l = ['' for _ in range(data.shape[0])]\n",
        "  for i in range(data.shape[0]):\n",
        "    for j in range(data.shape[1]):\n",
        "      l[i] += dic[data[i][j]]\n",
        "  return np.squeeze(l)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NC2DOn5Ekis"
      },
      "source": [
        "数据预处理:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqi66QVzqwhV",
        "outputId": "fc332bd4-d4ae-4a64-d7ba-3389764d476f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "def tok_tuple_yield(data, dic, axis=0):\n",
        "  for da in data:\n",
        "    l = tok(da[axis], dic)\n",
        "    yield (l, da[1]) if axis==0 else (da[0], l)\n",
        "\n",
        "input_pip = trax.data.Serial(\n",
        "  lambda _: load_dataset_yield(),\n",
        "  lambda x: tok_tuple_yield(x, human_vocab, 0),\n",
        "  lambda x: tok_tuple_yield(x, machine_vocab, 1),\n",
        "  trax.data.FilterByLength(max_length=max_len, length_keys=[0, 1]),\n",
        "  trax.data.BucketByLength(boundaries=[max_len],\n",
        "              batch_sizes=[batch_size, 1],\n",
        "              length_keys=[0, 1],\n",
        "              strict_pad_on_len=True),  \n",
        "  trax.data.AddLossWeights(id_to_mask=0),\n",
        ")\n",
        "\n",
        "train_batches_stream = input_pip()\n",
        "eval_batches_stream = input_pip()\n",
        "\n",
        "o = next(train_batches_stream)\n",
        "print(f'第一批数据:{o}')\n",
        "print(f'第一批数据tuple中每个数据shape: {[x.shape for x in o]}')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "第一批数据:(array([[ 6, 14, 14, ...,  0,  0,  0],\n",
            "       [ 6, 14, 14, ...,  0,  0,  0],\n",
            "       [ 6, 14, 13, ...,  0,  0,  0],\n",
            "       ...,\n",
            "       [24, 28,  2, ...,  0,  0,  0],\n",
            "       [ 6, 14, 14, ...,  0,  0,  0],\n",
            "       [ 6, 14, 12, ...,  0,  0,  0]]), array([[ 4, 12, 12, ...,  0,  0,  0],\n",
            "       [ 4, 12, 12, ...,  0,  0,  0],\n",
            "       [ 4, 12, 11, ...,  0,  0,  0],\n",
            "       ...,\n",
            "       [ 5,  3,  4, ...,  0,  0,  0],\n",
            "       [ 4, 12, 12, ...,  0,  0,  0],\n",
            "       [ 4, 12, 10, ...,  0,  0,  0]]), array([[1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.]], dtype=float32))\n",
            "第一批数据tuple中每个数据shape: [(64, 32), (64, 32), (64, 32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUyINmPXE-EF"
      },
      "source": [
        "创建Transformer训练模型:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoSz5plIyXOU"
      },
      "source": [
        "# MODEL\n",
        "def create_model(mode = 'train'):\n",
        "  return trax.models.Transformer(\n",
        "      input_vocab_size=len(human_vocab),\n",
        "      output_vocab_size=len(machine_vocab),\n",
        "      d_model=32, d_ff=128,\n",
        "      n_heads=8, n_encoder_layers=2, n_decoder_layers=2,\n",
        "      max_len=max_len, mode=mode)\n",
        "\n",
        "# UNUSED\n",
        "def create_model_reformer(mode = 'train'):\n",
        "  return trax.models.Reformer(input_vocab_size=8269,\n",
        "              output_vocab_size=8185,\n",
        "              d_model=256,\n",
        "              d_ff=1024,\n",
        "              n_encoder_layers=2,\n",
        "              n_decoder_layers=2,\n",
        "              n_heads=8,\n",
        "              dropout=0.1,\n",
        "              max_len=258,\n",
        "              ff_activation=tl.Relu,\n",
        "              ff_dropout=None,\n",
        "              mode=mode)\n",
        "          "
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TVx_BccFA6c"
      },
      "source": [
        "训练(使用GPU可以加快训练速度):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6bIKUO-3Cw8",
        "outputId": "6b263d2e-5607-4660-dbf4-42fc71ce6606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TRAIN\n",
        "from trax.supervised import training\n",
        "\n",
        "# Training task.\n",
        "train_task = training.TrainTask(\n",
        "    labeled_data=train_batches_stream,\n",
        "    loss_layer=tl.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adafactor(0.02),\n",
        "    #optimizer=trax.optimizers.Adam(learning_rate=0.1, weight_decay_rate=1e-05, b1=0.9, b2=0.98, eps=1e-06, clip_grad_norm=None),\n",
        "    n_steps_per_checkpoint=300,\n",
        ")\n",
        "\n",
        "# Evaluaton task.\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data=eval_batches_stream,\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        "    n_eval_batches=20  # For less variance in eval numbers.\n",
        ")\n",
        "\n",
        "# Training loop saves checkpoints to output_dir.\n",
        "output_dir = os.path.expanduser('~/output_dir/')\n",
        "print(output_dir)\n",
        "!rm -rf {output_dir}\n",
        "training_loop = training.Loop(create_model(),\n",
        "                train_task,\n",
        "                eval_tasks=[eval_task],\n",
        "                output_dir=output_dir)\n",
        "\n",
        "\n",
        "# Run\n",
        "training_loop.run(2400)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/output_dir/\n",
            "\n",
            "Step      1: Ran 1 train steps in 18.96 secs\n",
            "Step      1: train CrossEntropyLoss |  3.38156867\n",
            "Step      1: eval  CrossEntropyLoss |  2.55981293\n",
            "Step      1: eval          Accuracy |  0.13352273\n",
            "\n",
            "Step    300: Ran 299 train steps in 35.59 secs\n",
            "Step    300: train CrossEntropyLoss |  0.82325757\n",
            "Step    300: eval  CrossEntropyLoss |  0.47047343\n",
            "Step    300: eval          Accuracy |  0.83139207\n",
            "\n",
            "Step    600: Ran 300 train steps in 17.25 secs\n",
            "Step    600: train CrossEntropyLoss |  0.17830719\n",
            "Step    600: eval  CrossEntropyLoss |  0.06501543\n",
            "Step    600: eval          Accuracy |  0.97855116\n",
            "\n",
            "Step    900: Ran 300 train steps in 17.23 secs\n",
            "Step    900: train CrossEntropyLoss |  0.03004133\n",
            "Step    900: eval  CrossEntropyLoss |  0.01761373\n",
            "Step    900: eval          Accuracy |  0.99531253\n",
            "\n",
            "Step   1200: Ran 300 train steps in 17.19 secs\n",
            "Step   1200: train CrossEntropyLoss |  0.01196586\n",
            "Step   1200: eval  CrossEntropyLoss |  0.03649519\n",
            "Step   1200: eval          Accuracy |  0.99353697\n",
            "\n",
            "Step   1500: Ran 300 train steps in 17.14 secs\n",
            "Step   1500: train CrossEntropyLoss |  0.00855941\n",
            "Step   1500: eval  CrossEntropyLoss |  0.00779690\n",
            "Step   1500: eval          Accuracy |  0.99829547\n",
            "\n",
            "Step   1800: Ran 300 train steps in 17.20 secs\n",
            "Step   1800: train CrossEntropyLoss |  0.00715615\n",
            "Step   1800: eval  CrossEntropyLoss |  0.00399622\n",
            "Step   1800: eval          Accuracy |  0.99936080\n",
            "\n",
            "Step   2100: Ran 300 train steps in 17.11 secs\n",
            "Step   2100: train CrossEntropyLoss |  0.00504838\n",
            "Step   2100: eval  CrossEntropyLoss |  0.01915299\n",
            "Step   2100: eval          Accuracy |  0.99566764\n",
            "\n",
            "Step   2400: Ran 300 train steps in 17.18 secs\n",
            "Step   2400: train CrossEntropyLoss |  0.00390806\n",
            "Step   2400: eval  CrossEntropyLoss |  0.00465038\n",
            "Step   2400: eval          Accuracy |  0.99921876\n",
            "\n",
            "Step   2700: Ran 300 train steps in 17.14 secs\n",
            "Step   2700: train CrossEntropyLoss |  0.00389631\n",
            "Step   2700: eval  CrossEntropyLoss |  0.00296078\n",
            "Step   2700: eval          Accuracy |  0.99950285\n",
            "\n",
            "Step   3000: Ran 300 train steps in 17.16 secs\n",
            "Step   3000: train CrossEntropyLoss |  0.00337211\n",
            "Step   3000: eval  CrossEntropyLoss |  0.00233309\n",
            "Step   3000: eval          Accuracy |  0.99964489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtrqgebEM60O"
      },
      "source": [
        "测试模型效果:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B98rTQXeb2UJ",
        "outputId": "cdb97fdc-decb-43f8-9aad-283ee43722a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "for _ in range(10):\n",
        "  dat = next(load_dataset_yield())\n",
        "  # 模型必须每次重新加载, 因为调用autoregressive_sample()时,会改变model的状态值\n",
        "  model = create_model('predict') \n",
        "  model.init_from_file(output_dir+ '/model.pkl.gz', weights_only=True, \n",
        "            input_signature=[trax.shapes.ShapeDtype((1, 1), np.int32), \n",
        "                      trax.shapes.ShapeDtype((1, 1), np.int32),\n",
        "                      trax.shapes.ShapeDtype((1, 1), np.float32)])\n",
        "  # Tokenize a sentence.\n",
        "  test_source = dat[0]\n",
        "  test_target = dat[1]\n",
        "  test_tok = tok(test_source, human_vocab)\n",
        "  test_tok = test_tok[None, :]\n",
        "\n",
        "  res_tok = trax.supervised.decoding.autoregressive_sample(\n",
        "      model, inputs=test_tok, batch_size=1, temperature=0.5, \n",
        "      start_id=human_vocab['@'], eos_id=machine_vocab['#'], max_length=max_len, accelerate=False)\n",
        "\n",
        "  res = detok(res_tok, inv_machine_vocab)\n",
        "  print('======================================================')\n",
        "  print(f'输入: {test_source}')\n",
        "  print(f'token(输入): {test_tok}')\n",
        "  print(f'token(输出): {res_tok}')\n",
        "  print(f'输出: {res}')\n",
        "  print(f'真值: {test_target}')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================\n",
            "输入: 1999年2月9日星期二#\n",
            "token(输入): [[ 6 14 14 14 25  7 28 14 26 27 29 19  1]]\n",
            "token(输出): [[ 4 12 12 12  2  3  5  2  3 12  1]]\n",
            "输出: 1999-02-09#\n",
            "真值: 1999-02-09#\n",
            "======================================================\n",
            "输入: 1974年1月9日星期三#\n",
            "token(输入): [[ 6 14 12  9 25  6 28 14 26 27 29 17  1]]\n",
            "token(输出): [[ 4 12 10  7  2  3  4  2  3 12  1]]\n",
            "输出: 1974-01-09#\n",
            "真值: 1974-01-09#\n",
            "======================================================\n",
            "输入: 29 9月 1996#\n",
            "token(输入): [[ 7 14  2 14 28  2  6 14 14 11  1]]\n",
            "token(输出): [[ 4 12 12  9  2  3 12  2  5 12  1]]\n",
            "输出: 1996-09-29#\n",
            "真值: 1996-09-29#\n",
            "======================================================\n",
            "输入: 3 12月 1996#\n",
            "token(输入): [[ 8  2  6  7 28  2  6 14 14 11  1]]\n",
            "token(输出): [[ 4 12 12  9  2  4  5  2  3  6  1]]\n",
            "输出: 1996-12-03#\n",
            "真值: 1996-12-03#\n",
            "======================================================\n",
            "输入: 1978年7月9日星期日#\n",
            "token(输入): [[ 6 14 12 13 25 12 28 14 26 27 29 26  1]]\n",
            "token(输出): [[ 4 12 10 11  2  3 10  2  3 12  1]]\n",
            "输出: 1978-07-09#\n",
            "真值: 1978-07-09#\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}